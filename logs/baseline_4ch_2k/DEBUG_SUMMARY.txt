================================================================================
TETRIS AI TRAINING - DEBUG SUMMARY
================================================================================
Generated: 2025-11-05 22:43:11
Experiment: baseline_4ch_2k

================================================================================
TRAINING CONFIGURATION
================================================================================
Total episodes:        2000
Episodes trained:      2000 (from 0 to 2000)
Training time:         41.1 minutes (0.68 hours)
Time per episode:      1.23 seconds
Learning rate:         0.0005
Batch size:            64
Gamma (discount):      0.99
Epsilon start:         1.0
Epsilon end:           0.01
Epsilon decay:         0.9995
Final epsilon:         0.0200
Model type:            dqn
Complete vision:       True
CNN enabled:           True

================================================================================
CURRICULUM PROGRESSION
================================================================================
Final stage:           spreading_foundation

Stage Thresholds:
  Stage 1 (Foundation):        Episodes 0-500
  Stage 2 (Clean Placement):   Episodes 500-1000
  Stage 3 (Spreading Found):   Episodes 1000-2000
  Stage 4 (Clean Spreading):   Episodes 2000-5000
  Stage 5 (Line Clearing):     Episodes 5000+

Stage Transitions:
  Episode   500: foundation           â†’ clean_placement
  Episode  1000: clean_placement      â†’ spreading_foundation

================================================================================
PERFORMANCE METRICS (Last 100 Episodes)
================================================================================
Average reward:          -944.8
Average steps:             78.9
Average lines/ep:          0.02
Total lines cleared:         19
Overall lines/ep:         0.009
First line at:         Episode 1436

Board Quality Metrics:
  Holes:                   64.8  (target: <15)
  Columns used:             9.5/10  (target: â‰¥8)
  Completable rows:         0.0  (target: 3-5)
  Clean rows:               3.7  (target: 10-15)

================================================================================
SUCCESS CRITERIA EVALUATION
================================================================================
Stage 2 (Clean Placement):  âŒ FAILED
  Holes: 64.8 (target: <15)

Stage 4 (Clean Spreading):  âœ… SUCCESS
  Columns used: 9.5/10 (target: â‰¥8)

Stage 5 (Line Clearing):    âŒ FAILED
  Lines/episode: 0.02 (target: â‰¥2.0)

Overall Performance:        ðŸ“š NEEDS MORE TRAINING

================================================================================
PROBLEM ANALYSIS & RECOMMENDATIONS
================================================================================
Problems Detected:
  âŒ CRITICAL: Holes too high (>50) - Board is swiss cheese
  âœ… Spreading achieved (â‰¥8 columns)!
  âš ï¸  Line clears happening but infrequent

Recommendations:
  â†’ Agent not learning clean placement
  â†’ Increase hole penalty in current stage by 50%
  â†’ Reduce survival bonus if holes > 30
  â†’ Good progress, continue training

================================================================================
NEXT STEPS
================================================================================
ðŸ“š Training in progress. Agent still learning fundamentals.

Suggested next steps:
  1. Continue training to at least episode 5000 (3000 more episodes)
  2. Let curriculum fully progress through all 5 stages
  3. Monitor stage transitions and metrics

================================================================================
REWARD FUNCTION EFFECTIVENESS
================================================================================
Current stage: spreading_foundation

Typical reward for current performance (stage: spreading_foundation):
  Assumptions: 64 holes, 9 columns, 78 steps


If rewards are consistently negative:
  â†’ Agent is being penalized more than rewarded
  â†’ This is OK in early stages (learning from mistakes)
  â†’ Should become positive by episode 3000-5000

If rewards are too high (>10000) without line clears:
  â†’ Agent may be exploiting survival bonus
  â†’ Check if holes are high - survival should be conditional
  â†’ Hole penalty may need to be stronger

================================================================================
OUTPUT FILES
================================================================================
Log directory:         logs/baseline_4ch_2k
Board states:          board_states.txt
Episode log (CSV):     episode_log.csv
Reward plot:           reward_progress.png
Metrics plot:          training_metrics.png
Debug summary:         DEBUG_SUMMARY.txt (this file)

================================================================================
USEFUL DEBUG COMMANDS
================================================================================
View recent board states:
  tail -100 logs/baseline_4ch_2k/board_states.txt

Analyze hole progression:
  awk -F',' 'NR>1 {print $4,$6}' logs/baseline_4ch_2k/episode_log.csv | tail -100

Check line clearing progress:
  awk -F',' 'NR>1 {sum+=$7} NR%1000==0 {print NR,sum}' logs/baseline_4ch_2k/episode_log.csv

Resume training:
  python train_progressive_improved.py --episodes 7000 --resume

================================================================================
END OF DEBUG SUMMARY
================================================================================