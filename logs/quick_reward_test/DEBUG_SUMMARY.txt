================================================================================
TETRIS AI TRAINING - DEBUG SUMMARY
================================================================================
Generated: 2025-11-07 21:32:57
Experiment: quick_reward_test

================================================================================
TRAINING CONFIGURATION
================================================================================
Total episodes:        100
Episodes trained:      100 (from 0 to 100)
Training time:         0.7 minutes (0.01 hours)
Time per episode:      0.44 seconds
Learning rate:         0.0005
Batch size:            64
Gamma (discount):      0.99
Epsilon start:         1.0
Epsilon end:           0.01
Epsilon decay:         0.9995
Final epsilon:         0.0200
Model type:            hybrid_dqn
Complete vision:       True
CNN enabled:           True

================================================================================
CURRICULUM PROGRESSION
================================================================================
Final stage:           foundation

Stage Thresholds:
  Stage 1 (Foundation):        Episodes 0-500
  Stage 2 (Clean Placement):   Episodes 500-1000
  Stage 3 (Spreading Found):   Episodes 1000-2000
  Stage 4 (Clean Spreading):   Episodes 2000-5000
  Stage 5 (Line Clearing):     Episodes 5000+

Stage Transitions: None (training too short or didn't advance)

================================================================================
PERFORMANCE METRICS (Last 100 Episodes)
================================================================================
Average reward:          1780.3
Average steps:             66.0
Average lines/ep:          0.00
Total lines cleared:          0
Overall lines/ep:         0.000
First line at:         Episode Never

Board Quality Metrics:
  Holes:                   29.2  (target: <15)
  Columns used:             5.8/10  (target: â‰¥8)
  Completable rows:         0.0  (target: 3-5)
  Clean rows:               2.5  (target: 10-15)

================================================================================
SUCCESS CRITERIA EVALUATION
================================================================================
Stage 2 (Clean Placement):  âŒ FAILED
  Holes: 29.2 (target: <15)

Stage 4 (Clean Spreading):  âŒ FAILED
  Columns used: 5.8/10 (target: â‰¥8)

Stage 5 (Line Clearing):    âŒ FAILED
  Lines/episode: 0.00 (target: â‰¥2.0)

Overall Performance:        ðŸ“š NEEDS MORE TRAINING

================================================================================
PROBLEM ANALYSIS & RECOMMENDATIONS
================================================================================
Problems Detected:
  âš ï¸  Holes moderate (15-30) - Room for improvement
  âŒ CRITICAL: Not spreading (<6 columns) - Center stacking
  âš ï¸  Line clears happening but infrequent

Recommendations:
  â†’ Agent progressing but not mastered clean play
  â†’ Continue training in current stage
  â†’ Increase spread bonus and columns_used reward
  â†’ Increase outer_unused penalty
  â†’ Check if Stage 3+ rewards are active
  â†’ Good progress, continue training

================================================================================
NEXT STEPS
================================================================================
ðŸ“š Training in progress. Agent still learning fundamentals.

Suggested next steps:
  1. Continue training to at least episode 5000 (4900 more episodes)
  2. Let curriculum fully progress through all 5 stages
  3. Monitor stage transitions and metrics

================================================================================
REWARD FUNCTION EFFECTIVENESS
================================================================================
Current stage: foundation

Typical reward for current performance (stage: foundation):
  Assumptions: 29 holes, 5 columns, 66 steps


If rewards are consistently negative:
  â†’ Agent is being penalized more than rewarded
  â†’ This is OK in early stages (learning from mistakes)
  â†’ Should become positive by episode 3000-5000

If rewards are too high (>10000) without line clears:
  â†’ Agent may be exploiting survival bonus
  â†’ Check if holes are high - survival should be conditional
  â†’ Hole penalty may need to be stronger

================================================================================
OUTPUT FILES
================================================================================
Log directory:         logs/quick_reward_test
Board states:          board_states.txt
Episode log (CSV):     episode_log.csv
Reward plot:           reward_progress.png
Metrics plot:          training_metrics.png
Debug summary:         DEBUG_SUMMARY.txt (this file)

================================================================================
USEFUL DEBUG COMMANDS
================================================================================
View recent board states:
  tail -100 logs/quick_reward_test/board_states.txt

Analyze hole progression:
  awk -F',' 'NR>1 {print $4,$6}' logs/quick_reward_test/episode_log.csv | tail -100

Check line clearing progress:
  awk -F',' 'NR>1 {sum+=$7} NR%1000==0 {print NR,sum}' logs/quick_reward_test/episode_log.csv

Resume training:
  python train_progressive_improved.py --episodes 5100 --resume

================================================================================
END OF DEBUG SUMMARY
================================================================================