================================================================================
TETRIS AI TRAINING - DEBUG SUMMARY
================================================================================
Generated: 2025-11-07 12:45:38
Experiment: hybrid_15k_fixed_curriculum

================================================================================
TRAINING CONFIGURATION
================================================================================
Total episodes:        7500
Episodes trained:      7500 (from 0 to 7500)
Training time:         321.3 minutes (5.36 hours)
Time per episode:      2.57 seconds
Learning rate:         0.0005
Batch size:            64
Gamma (discount):      0.99
Epsilon start:         1.0
Epsilon end:           0.01
Epsilon decay:         0.9995
Final epsilon:         0.3057
Model type:            hybrid_dqn
Complete vision:       True
CNN enabled:           True

================================================================================
CURRICULUM PROGRESSION
================================================================================
Final stage:           clean_spreading

Stage Thresholds:
  Stage 1 (Foundation):        Episodes 0-500
  Stage 2 (Clean Placement):   Episodes 500-1000
  Stage 3 (Spreading Found):   Episodes 1000-2000
  Stage 4 (Clean Spreading):   Episodes 2000-5000
  Stage 5 (Line Clearing):     Episodes 5000+

Stage Transitions:
  Episode   500: foundation           â†’ clean_placement
  Episode  1000: clean_placement      â†’ spreading_foundation
  Episode  2000: spreading_foundation â†’ clean_spreading

================================================================================
PERFORMANCE METRICS (Last 100 Episodes)
================================================================================
Average reward:           324.9
Average steps:            204.7
Average lines/ep:          0.07
Total lines cleared:        272
Overall lines/ep:         0.036
First line at:         Episode 1662

Board Quality Metrics:
  Holes:                   30.3  (target: <15)
  Columns used:            10.0/10  (target: â‰¥8)
  Completable rows:         0.1  (target: 3-5)
  Clean rows:               1.1  (target: 10-15)

================================================================================
SUCCESS CRITERIA EVALUATION
================================================================================
Stage 2 (Clean Placement):  âŒ FAILED
  Holes: 30.3 (target: <15)

Stage 4 (Clean Spreading):  âœ… SUCCESS
  Columns used: 10.0/10 (target: â‰¥8)

Stage 5 (Line Clearing):    âŒ FAILED
  Lines/episode: 0.07 (target: â‰¥2.0)

Overall Performance:        ðŸ‘ GOOD PROGRESS

================================================================================
PROBLEM ANALYSIS & RECOMMENDATIONS
================================================================================
Problems Detected:
  âš ï¸  WARNING: Holes high (30-50) - Board quality poor
  âœ… Spreading achieved (â‰¥8 columns)!
  âŒ CRITICAL: No line clears despite 5000+ episodes
  âš ï¸  WARNING: No completable rows (rows with 8-9 filled, no holes)
  âš ï¸  WARNING: Building tall towers with many holes

Recommendations:
  â†’ Agent needs more time in earlier stages
  â†’ Consider increasing hole penalty by 20-30%
  â†’ Agent likely has too many holes to clear lines
  â†’ Check completable_rows metric (should be >0)
  â†’ May need to restart with stronger hole penalties
  â†’ Agent not learning to set up line clears
  â†’ Increase completable_rows bonus significantly
  â†’ This is the key metric bridging placement â†’ line clears
  â†’ Agent getting survival bonus despite bad board
  â†’ Make survival bonus more conditional (only if holes <20)
  â†’ Add explicit height penalty for towers >15

================================================================================
NEXT STEPS
================================================================================
ðŸ‘ Good progress! Agent has learned spreading.

Suggested next steps:
  1. Continue training for 3000 more episodes
  2. Focus on reducing holes to enable line clears
  3. Monitor completable_rows metric - should increase
  4. Expect first consistent line clears around episode 6000-8000

================================================================================
REWARD FUNCTION EFFECTIVENESS
================================================================================
Current stage: clean_spreading

Typical reward for current performance (stage: clean_spreading):
  Assumptions: 30 holes, 9 columns, 204 steps

  Hole penalty:        -2.5 Ã— 30 = -75.7
  Completable rows:    +10.0 Ã— 0.1 = 1.1
  Clean rows:          +7.0 Ã— 1.1 = 7.9
  Spread bonus:        ~+25.0
  Columns bonus:       +5.0 Ã— 10 = 50.0

If rewards are consistently negative:
  â†’ Agent is being penalized more than rewarded
  â†’ This is OK in early stages (learning from mistakes)
  â†’ Should become positive by episode 3000-5000

If rewards are too high (>10000) without line clears:
  â†’ Agent may be exploiting survival bonus
  â†’ Check if holes are high - survival should be conditional
  â†’ Hole penalty may need to be stronger

================================================================================
OUTPUT FILES
================================================================================
Log directory:         logs/hybrid_15k_fixed_curriculum
Board states:          board_states.txt
Episode log (CSV):     episode_log.csv
Reward plot:           reward_progress.png
Metrics plot:          training_metrics.png
Debug summary:         DEBUG_SUMMARY.txt (this file)

================================================================================
USEFUL DEBUG COMMANDS
================================================================================
View recent board states:
  tail -100 logs/hybrid_15k_fixed_curriculum/board_states.txt

Analyze hole progression:
  awk -F',' 'NR>1 {print $4,$6}' logs/hybrid_15k_fixed_curriculum/episode_log.csv | tail -100

Check line clearing progress:
  awk -F',' 'NR>1 {sum+=$7} NR%1000==0 {print NR,sum}' logs/hybrid_15k_fixed_curriculum/episode_log.csv

Resume training:
  python train_progressive_improved.py --episodes 12500 --resume

================================================================================
END OF DEBUG SUMMARY
================================================================================