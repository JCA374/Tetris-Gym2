================================================================================
TETRIS AI TRAINING - DEBUG SUMMARY
================================================================================
Generated: 2025-11-05 22:51:27
Experiment: hybrid_8ch_2k

================================================================================
TRAINING CONFIGURATION
================================================================================
Total episodes:        2000
Episodes trained:      2000 (from 0 to 2000)
Training time:         49.2 minutes (0.82 hours)
Time per episode:      1.48 seconds
Learning rate:         0.0005
Batch size:            64
Gamma (discount):      0.99
Epsilon start:         1.0
Epsilon end:           0.01
Epsilon decay:         0.9995
Final epsilon:         0.0200
Model type:            dqn
Complete vision:       True
CNN enabled:           True

================================================================================
CURRICULUM PROGRESSION
================================================================================
Final stage:           spreading_foundation

Stage Thresholds:
  Stage 1 (Foundation):        Episodes 0-500
  Stage 2 (Clean Placement):   Episodes 500-1000
  Stage 3 (Spreading Found):   Episodes 1000-2000
  Stage 4 (Clean Spreading):   Episodes 2000-5000
  Stage 5 (Line Clearing):     Episodes 5000+

Stage Transitions:
  Episode   500: foundation           â†’ clean_placement
  Episode  1000: clean_placement      â†’ spreading_foundation

================================================================================
PERFORMANCE METRICS (Last 100 Episodes)
================================================================================
Average reward:          4009.4
Average steps:            149.6
Average lines/ep:          0.06
Total lines cleared:         35
Overall lines/ep:         0.018
First line at:         Episode 1443

Board Quality Metrics:
  Holes:                   51.7  (target: <15)
  Columns used:             9.9/10  (target: â‰¥8)
  Completable rows:         0.1  (target: 3-5)
  Clean rows:               2.9  (target: 10-15)

================================================================================
SUCCESS CRITERIA EVALUATION
================================================================================
Stage 2 (Clean Placement):  âŒ FAILED
  Holes: 51.7 (target: <15)

Stage 4 (Clean Spreading):  âœ… SUCCESS
  Columns used: 9.9/10 (target: â‰¥8)

Stage 5 (Line Clearing):    âŒ FAILED
  Lines/episode: 0.06 (target: â‰¥2.0)

Overall Performance:        ðŸ‘ GOOD PROGRESS

================================================================================
PROBLEM ANALYSIS & RECOMMENDATIONS
================================================================================
Problems Detected:
  âŒ CRITICAL: Holes too high (>50) - Board is swiss cheese
  âœ… Spreading achieved (â‰¥8 columns)!
  âš ï¸  Line clears happening but infrequent
  âš ï¸  WARNING: Building tall towers with many holes

Recommendations:
  â†’ Agent not learning clean placement
  â†’ Increase hole penalty in current stage by 50%
  â†’ Reduce survival bonus if holes > 30
  â†’ Good progress, continue training
  â†’ Agent getting survival bonus despite bad board
  â†’ Make survival bonus more conditional (only if holes <20)
  â†’ Add explicit height penalty for towers >15

================================================================================
NEXT STEPS
================================================================================
ðŸ‘ Good progress! Agent has learned spreading.

Suggested next steps:
  1. Continue training for 8000 more episodes
  2. Focus on reducing holes to enable line clears
  3. Monitor completable_rows metric - should increase
  4. Expect first consistent line clears around episode 6000-8000

================================================================================
REWARD FUNCTION EFFECTIVENESS
================================================================================
Current stage: spreading_foundation

Typical reward for current performance (stage: spreading_foundation):
  Assumptions: 51 holes, 9 columns, 149 steps


If rewards are consistently negative:
  â†’ Agent is being penalized more than rewarded
  â†’ This is OK in early stages (learning from mistakes)
  â†’ Should become positive by episode 3000-5000

If rewards are too high (>10000) without line clears:
  â†’ Agent may be exploiting survival bonus
  â†’ Check if holes are high - survival should be conditional
  â†’ Hole penalty may need to be stronger

================================================================================
OUTPUT FILES
================================================================================
Log directory:         logs/hybrid_8ch_2k
Board states:          board_states.txt
Episode log (CSV):     episode_log.csv
Reward plot:           reward_progress.png
Metrics plot:          training_metrics.png
Debug summary:         DEBUG_SUMMARY.txt (this file)

================================================================================
USEFUL DEBUG COMMANDS
================================================================================
View recent board states:
  tail -100 logs/hybrid_8ch_2k/board_states.txt

Analyze hole progression:
  awk -F',' 'NR>1 {print $4,$6}' logs/hybrid_8ch_2k/episode_log.csv | tail -100

Check line clearing progress:
  awk -F',' 'NR>1 {sum+=$7} NR%1000==0 {print NR,sum}' logs/hybrid_8ch_2k/episode_log.csv

Resume training:
  python train_progressive_improved.py --episodes 7000 --resume

================================================================================
END OF DEBUG SUMMARY
================================================================================