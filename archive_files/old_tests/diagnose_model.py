# diagnose_model.py
"""Diagnose why the model only outputs NOOP"""

import sys
import os
from pathlib import Path

# Add parent directory to path so we can import config and src modules
parent_dir = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(parent_dir))

import torch
import numpy as np
from config import make_env
from src.agent import Agent


def diagnose_q_network(model_path):
    """Check what Q-values the network is producing"""
    
    print("="*80)
    print("üîç DIAGNOSING Q-NETWORK")
    print("="*80)
    
    # Create environment and agent
    env = make_env(render_mode=None)
    agent = Agent(
        obs_space=env.observation_space,
        action_space=env.action_space,
        model_type='dqn'
    )
    
    # Load checkpoint
    agent.load_checkpoint(path=model_path)
    print(f"\n‚úÖ Loaded checkpoint: {model_path}")
    print(f"   Episode: {agent.episodes_done}")
    print(f"   Epsilon: {agent.epsilon:.6f}")
    print(f"   Steps: {agent.steps_done}")
    
    # Get a sample observation
    obs, _ = env.reset()
    print(f"\nüìä Observation shape: {obs.shape}")
    print(f"   Observation range: [{obs.min():.3f}, {obs.max():.3f}]")
    
    # Check Q-values
    print("\nüß† Q-NETWORK OUTPUT ANALYSIS")
    print("-"*80)
    
    agent.q_network.eval()
    with torch.no_grad():
        state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(agent.device)
        q_values = agent.q_network(state_tensor)
        q_values = q_values.cpu().numpy()[0]
    
    print(f"\nQ-values for all actions:")
    action_names = ['NOOP', 'LEFT', 'RIGHT', 'DOWN', 'ROTATE_CW', 'ROTATE_CCW', 'HARD_DROP', 'SWAP']
    
    for i, (name, q_val) in enumerate(zip(action_names, q_values)):
        marker = " ‚Üê SELECTED" if i == q_values.argmax() else ""
        print(f"   Action {i} ({name:11s}): Q = {q_val:8.4f}{marker}")
    
    # Statistics
    print(f"\nüìà Q-VALUE STATISTICS:")
    print(f"   Mean:    {q_values.mean():.4f}")
    print(f"   Std Dev: {q_values.std():.4f}")
    print(f"   Min:     {q_values.min():.4f}")
    print(f"   Max:     {q_values.max():.4f}")
    print(f"   Range:   {q_values.max() - q_values.min():.4f}")
    
    # Check for problems
    print("\nüîç DIAGNOSTIC CHECKS:")
    print("-"*80)
    
    issues = []
    
    # Check 1: All Q-values near zero (not trained)
    if abs(q_values.mean()) < 0.1 and q_values.std() < 0.1:
        issues.append("‚ùå Q-values near zero - Network hasn't learned anything")
        issues.append("   ‚Üí Need more training episodes (try 2000+)")
    
    # Check 2: Q-values all similar (no differentiation)
    if q_values.std() < 0.01:
        issues.append("‚ùå Q-values too similar - Network can't distinguish actions")
        issues.append("   ‚Üí Check if network is getting gradients")
        issues.append("   ‚Üí Verify reward shaping is providing learning signal")
    
    # Check 3: Q-values all very negative
    if q_values.max() < -10:
        issues.append("‚ùå All Q-values very negative - Agent expects failure")
        issues.append("   ‚Üí Agent never experienced positive rewards")
        issues.append("   ‚Üí Check reward shaping function")
    
    # Check 4: Only one action has high Q-value
    if q_values.argmax() == 0 and q_values[0] > q_values[1:].max() + 0.1:
        issues.append("‚ùå Only NOOP has high Q-value")
        issues.append("   ‚Üí Agent learned to do nothing")
        issues.append("   ‚Üí Check exploration is actually happening during training")
    
    # Check 5: Q-values exploded (too large)
    if abs(q_values.max()) > 1000:
        issues.append("‚ùå Q-values too large - Possible training instability")
        issues.append("   ‚Üí Reduce learning rate")
        issues.append("   ‚Üí Add gradient clipping")
    
    if issues:
        print("\nüö® ISSUES FOUND:")
        for issue in issues:
            print(issue)
    else:
        print("‚úÖ Q-values look reasonable")
    
    # Test on multiple states
    print("\n\nüé≤ TESTING ON 10 RANDOM STATES")
    print("-"*80)
    
    action_selections = {i: 0 for i in range(len(action_names))}
    
    for i in range(10):
        obs, _ = env.reset(seed=i)
        with torch.no_grad():
            state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(agent.device)
            q_values = agent.q_network(state_tensor)
            action = q_values.argmax().item()
            action_selections[action] += 1
    
    print("\nAction selections across 10 different states:")
    for i, name in enumerate(action_names):
        count = action_selections[i]
        pct = (count / 10) * 100
        bar = "‚ñà" * count
        print(f"   {i} {name:11s}: {bar:10s} {count}/10 ({pct:.0f}%)")
    
    if action_selections[0] == 10:
        print("\n‚ùå ALWAYS SELECTING NOOP - This is the problem!")
        print("\nüí° POSSIBLE CAUSES:")
        print("   1. Network weights are still random/uninitialized")
        print("   2. Network learned that NOOP is safest (negative rewards for everything else)")
        print("   3. Training hasn't run long enough (only 500 episodes)")
        print("   4. Exploration not happening during training")
        print("\nüí° SOLUTIONS:")
        print("   1. Train for more episodes: python train.py --episodes 2000")
        print("   2. Check training logs for learning progress")
        print("   3. Verify exploration is using all actions during training")
        print("   4. Check reward shaping is providing positive feedback")
    
    env.close()
    
    return q_values, action_selections


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Diagnose Q-network behavior')
    parser.add_argument('--model', type=str, default='models/checkpoint_latest.pth',
                        help='Path to model checkpoint')
    args = parser.parse_args()
    
    diagnose_q_network(args.model)